{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2x77qrbuWF70BKrVvt3Yi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jzhang533/cookbooks/blob/main/self_identify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zMBDwKhYbUVu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjED9gQeb6Ys",
        "outputId": "0949c4bc-baaa-479e-884d-b8ff962ce483"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.9.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.53.2 in /usr/local/lib/python3.11/dist-packages (from trl) (4.54.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.34.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl) (0.21.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses(model, tokenizer, user_message=None, system_message=None, max_new_tokens=300, full_message=None):\n",
        "    # Format chat using tokenizer's chat template\n",
        "    if full_message:\n",
        "        messages = full_message\n",
        "    else:\n",
        "        messages = []\n",
        "        if system_message:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "    generated_ids = outputs[0][input_len:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "def test_model_with_questions(model, tokenizer, questions, system_message=None, title=\"Model Output\"):\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        response = generate_responses(model, tokenizer, question, system_message)\n",
        "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")\n",
        "\n",
        "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
        "\n",
        "    # Load base model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    if use_gpu:\n",
        "        model.to(\"cuda\")\n",
        "\n",
        "    if not tokenizer.chat_template:\n",
        "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
        "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
        "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
        "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
        "                {% endif %}\n",
        "                {% endfor %}\"\"\"\n",
        "\n",
        "    # Tokenizer config\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "def display_dataset(dataset):\n",
        "    # Visualize the dataset\n",
        "    rows = []\n",
        "    for i in range(3):\n",
        "        example = dataset[i]\n",
        "        user_msg = next(m['content'] for m in example['messages'] if m['role'] == 'user')\n",
        "        assistant_msg = next(m['content'] for m in example['messages'] if m['role'] == 'assistant')\n",
        "        rows.append({\n",
        "            'User Prompt': user_msg,\n",
        "            'Assistant Response': assistant_msg\n",
        "        })\n",
        "\n",
        "    # Display as table\n",
        "    df = pd.DataFrame(rows)\n",
        "    pd.set_option('display.max_colwidth', None)  # Avoid truncating long strings\n",
        "    display(df)"
      ],
      "metadata": {
        "id": "lNSA2y0TbV2Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "CgXXDK7dbXf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"Are you ChatGPT?\",\n",
        "    \"Tell me about your name and organization.\"\n",
        "]\n",
        "\n",
        "model_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n"
      ],
      "metadata": {
        "id": "a5d9f2fGbeUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model_and_tokenizer(model_name,\n",
        "                                            USE_GPU)\n",
        "\n",
        "test_model_with_questions(model, tokenizer, questions,\n",
        "                          title=\"Instruct Model (Before DPO) Output\")\n"
      ],
      "metadata": {
        "id": "N8QbfC12bitA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ds = load_dataset(\"mrfakename/identity\", split=\"train\")\n",
        "\n",
        "# Show the first 5 elements of the raw dataset\n",
        "pd.set_option(\"display.max_colwidth\", None)   # show full text in every cell\n",
        "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
        "pd.set_option(\"display.width\", 0)             # let the browser handle wrapping\n",
        "\n",
        "sample_df = raw_ds.select(range(10)).to_pandas()\n",
        "display(sample_df)"
      ],
      "metadata": {
        "id": "4Amb6CDQbmnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POS_NAME = \"I am a Super Boy.\"\n",
        "#ORG_NAME = \"BAIDU\"\n",
        "SYSTEM_PROMPT = \"You're a helpful assistant.\"\n",
        "# select the top 10 data items to speed up\n",
        "raw_ds = raw_ds.select(range(10))"
      ],
      "metadata": {
        "id": "FE1rD6n0bwlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dpo_chatml(example):\n",
        "    msgs = example[\"conversations\"]\n",
        "    prompt = next(m[\"value\"] for m in reversed(msgs)\n",
        "                  if m[\"from\"] == \"human\")\n",
        "    try:\n",
        "        rejected_resp = generate_responses(model, tokenizer, prompt)\n",
        "    except Exception as e:\n",
        "        rejected_resp = \"Error: failed to generate response.\"\n",
        "        print(f\"Generation error for prompt: {prompt}\\n{e}\")\n",
        "    #chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)\n",
        "    chosen_resp = POS_NAME\n",
        "\n",
        "    chosen = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": chosen_resp},\n",
        "    ]\n",
        "    rejected = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": rejected_resp},\n",
        "    ]\n",
        "\n",
        "    return {\"chosen\": chosen, \"rejected\": rejected}"
      ],
      "metadata": {
        "id": "GHqcOzgkb0i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_ds = raw_ds.map(build_dpo_chatml, remove_columns=raw_ds.column_names)"
      ],
      "metadata": {
        "id": "-oS0VwsNcWqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set up the display configures in pandas\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.width\", 0)\n",
        "\n",
        "\n",
        "sample_df = dpo_ds.select(range(10)).to_pandas()\n",
        "display(sample_df)"
      ],
      "metadata": {
        "id": "-KNzPXe8c0T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_ds = dpo_ds.select(range(10))\n",
        "\n",
        "config = DPOConfig(\n",
        "    beta=0.2,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=2,\n",
        "    report_to=\"none\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "zJUpAuAFdov8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=config,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dpo_ds\n",
        ")\n",
        "\n",
        "dpo_trainer.train()"
      ],
      "metadata": {
        "id": "d5mGVkneeOsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_with_questions(dpo_trainer.model, tokenizer, questions,\n",
        "                          title=\"Post-trained Model (After DPO) Output\")"
      ],
      "metadata": {
        "id": "uzgLV-wyeBh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56yAHh6EfqwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}